{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omrdatasettools.Downloader import Downloader\n",
    "from omrdatasettools.OmrDataset import OmrDataset\n",
    "from omrdatasettools.MuscimaPlusPlusSymbolImageGenerator import MuscimaPlusPlusSymbolImageGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloader module is used to extract the raw information about the datasets into the ./data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = Downloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.download_and_extract_dataset(OmrDataset.MuscimaPlusPlus_V2, \"data/muscima_pp_v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated raw files are then used to draw the images of the dataset on canvases and store them in different folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muscima_generator = MuscimaPlusPlusSymbolImageGenerator()\n",
    "muscima_generator.extract_and_render_all_symbol_masks(\"./data/muscima_pp_v2/\", \"./data/muscima_pp_v2_symbols/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models, the images first have to be resized to be a standard size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find out what the max height and width is, and use those values to resize all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Specify the root folder containing your images\n",
    "root_folder = \".\\\\data\\\\muscima_pp_v2_symbols\"\n",
    "\n",
    "# Desired width to search for\n",
    "desired_width = 861\n",
    "\n",
    "# Initialize a list to store image file paths that match the desired width\n",
    "matching_images = []\n",
    "\n",
    "# Iterate through all subfolders and their images\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            image = Image.open(image_path)\n",
    "            _, height = image.size\n",
    "            if height == desired_width:\n",
    "                matching_images.append(image_path)\n",
    "\n",
    "# Print the paths of images with the desired width\n",
    "if matching_images:\n",
    "    print(\"Images with a width of 1153 pixels:\")\n",
    "    for image_path in matching_images:\n",
    "        print(image_path)\n",
    "else:\n",
    "    print(\"No images with a width of 1153 pixels found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images in subfolders have been resized to the maximum width and height.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Specify the root folder containing subfolders with images\n",
    "root_folder = \".\\\\data\\\\muscima_pp_v2_symbols\"\n",
    "\n",
    "# Initialize variables to store maximum width and height\n",
    "max_width = 0\n",
    "max_height = 0\n",
    "\n",
    "# Iterate through all subfolders and their images to find maximum width and height\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            max_width = max(max_width, width)\n",
    "            max_height = max(max_height, height)\n",
    "\n",
    "# Iterate through all subfolders and their images again and pad them\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Calculate padding dimensions\n",
    "            padding_width = max_width - image.width\n",
    "            padding_height = max_height - image.height\n",
    "            \n",
    "            # Create a new image with white padding\n",
    "            padded_image = Image.new(\"RGB\", (max_width, max_height), (255, 255, 255))\n",
    "            \n",
    "            # Calculate the position to paste the original image (centered)\n",
    "            paste_x = padding_width // 2\n",
    "            paste_y = padding_height // 2\n",
    "            \n",
    "            # Paste the original image onto the padded image\n",
    "            padded_image.paste(image, (paste_x, paste_y))\n",
    "            \n",
    "            # Save the padded image, overwriting the original\n",
    "            padded_image.save(image_path)\n",
    "\n",
    "print(\"All images in subfolders have been resized to the maximum width and height.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the images are quite large. To aid in training, we can reduce the size by 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images in subfolders have been resized to 50% of their size while preserving aspect ratio and stored in corresponding output subfolders.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Specify the root folder containing the padded images\n",
    "root_folder = \".\\\\data\\\\muscima_pp_v2_symbols\"\n",
    "\n",
    "# Initialize the output folder\n",
    "output_root_folder = \".\\\\data\\\\muscima_pp_v2_symbols_resized\"\n",
    "\n",
    "# Create the output root folder if it doesn't exist\n",
    "if not os.path.exists(output_root_folder):\n",
    "    os.makedirs(output_root_folder)\n",
    "\n",
    "# Specify the scale factor for resizing (50% in this case)\n",
    "scale_factor = 0.5\n",
    "\n",
    "# Iterate through all subfolders and their images\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    # Create the corresponding subfolder structure in the output directory\n",
    "    relative_folder = os.path.relpath(foldername, root_folder)\n",
    "    output_folder = os.path.join(output_root_folder, relative_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # Calculate the new dimensions while preserving aspect ratio\n",
    "            width, height = image.size\n",
    "            new_width = int(width * scale_factor)\n",
    "            new_height = int(height * scale_factor)\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = image.resize((new_width, new_height))\n",
    "\n",
    "            # Save the resized image to the corresponding output subfolder\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            resized_image.save(output_path)\n",
    "\n",
    "print(\"All images in subfolders have been resized to 50% of their size while preserving aspect ratio and stored in corresponding output subfolders.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to supplement the dataset by rotating the images 10, -10, 15 and -15 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_folder = \".\\\\data\\\\muscima_pp_v2_symbols_resized\"\n",
    "\n",
    "output_root_folder = \".\\\\data\\\\muscima_pp_v2_symbols_resized_rotated\"\n",
    "\n",
    "# This list can be extended with more degrees as needed\n",
    "rotate_degrees = [10, 15, -10, -15] \n",
    "\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    # Create the corresponding subfolder structure in the output directory\n",
    "    relative_folder = os.path.relpath(foldername, root_folder)\n",
    "    output_folder = os.path.join(output_root_folder, relative_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            # Get the full image path\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # Get image dimensions\n",
    "            height, width = image.size\n",
    "\n",
    "            # Save the original image to the output folder\n",
    "            original_image_path = os.path.join(output_folder, filename)\n",
    "            image.save(original_image_path)\n",
    "\n",
    "            for degree in rotate_degrees:\n",
    "                new_image = image.rotate(degree, fillcolor=(255, 255, 255))\n",
    "\n",
    "                # Get the original file extension\n",
    "                file_extension = os.path.splitext(filename)[-1]\n",
    "\n",
    "                # Save the rotated image with a new filename in the output folder\n",
    "                new_filename = filename.replace(file_extension, f\"_rotated{degree}{file_extension}\")\n",
    "                new_image_path = os.path.join(output_folder, new_filename)\n",
    "                new_image.save(new_image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be useful to randomly add images where some features are missing to the training dataset, to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import os\n",
    "\n",
    "root_folder = \".\\\\data\\\\train\"\n",
    "\n",
    "output_root_folder = \".\\\\data\\\\train_cut\"\n",
    "\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    x = 10  # Change this to the desired number of random numbers, i.e., the number of images to cut\n",
    "\n",
    "    # Generate x random numbers between 0 and 99\n",
    "    random_numbers = [random.randint(0, 99) for _ in range(x)]\n",
    "    \n",
    "    # Create the corresponding subfolder structure in the output directory\n",
    "    relative_folder = os.path.relpath(foldername, root_folder)\n",
    "    output_folder = os.path.join(output_root_folder, relative_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if (filenames.index(filename) in random_numbers) and filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            # Get the full image path\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            # Open the image\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # Get the dimensions of the image\n",
    "            width, height = image.size\n",
    "            \n",
    "            # Save the original image to the output folder\n",
    "            original_image_path = os.path.join(output_folder, filename)\n",
    "            image.save(original_image_path)\n",
    "\n",
    "            # Create a new blank image with the same size and white background\n",
    "            output_image = Image.new(\"RGB\", (width, height), \"white\")\n",
    "\n",
    "            # Create a draw object to manipulate the new image\n",
    "            draw = ImageDraw.Draw(output_image)\n",
    "\n",
    "            # Define the coordinates for the top half of the image\n",
    "            top_half = (0, 0, width, height // 2)\n",
    "\n",
    "            # Paste the top half of the input image onto the new image\n",
    "            output_image.paste(image.crop(top_half))\n",
    "\n",
    "            file_extension = os.path.splitext(filename)[-1]\n",
    "\n",
    "            # Save the cut image with a new filename in the output folder\n",
    "            new_filename = filename.replace(file_extension, f\"_cut{file_extension}\")\n",
    "            new_image_path = os.path.join(output_folder, new_filename)\n",
    "            # Save the resulting image\n",
    "            output_image.save(new_image_path)\n",
    "\n",
    "            # Close the input and output images\n",
    "            image.close()\n",
    "            output_image.close()\n",
    "        else:\n",
    "            # Get the full image path\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            # Open the image\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Save the original image to the output folder\n",
    "            original_image_path = os.path.join(output_folder, filename)\n",
    "            image.save(original_image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salt and Pepper noise can also be added to a random selection of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "root_folder = \".\\\\data\\\\train_cut\"\n",
    "\n",
    "output_root_folder = \".\\\\data\\\\train_salt_and_pepper\"\n",
    "\n",
    "for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "    x = 10  # Change this to the desired number of random numbers, i.e., the number of images to cut\n",
    "\n",
    "    # Generate x random numbers between 0 and 99\n",
    "    random_numbers = [random.randint(0, 99) for _ in range(x)]\n",
    "    \n",
    "    # Create the corresponding subfolder structure in the output directory\n",
    "    relative_folder = os.path.relpath(foldername, root_folder)\n",
    "    output_folder = os.path.join(output_root_folder, relative_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if (filenames.index(filename) in random_numbers) and filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n",
    "            # Get the full image path\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            # Open the image\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Save the original image to the output folder\n",
    "            original_image_path = os.path.join(output_folder, filename)\n",
    "            image.save(original_image_path)\n",
    "\n",
    "            # Convert the image to a numpy array for easy manipulation\n",
    "            image_array = np.array(image)\n",
    "\n",
    "            # Define the probability of adding salt and pepper noise\n",
    "            noise_probability = 0.02  # You can adjust this value\n",
    "\n",
    "            # Get the dimensions of the image\n",
    "            height, width, channels = image_array.shape\n",
    "\n",
    "            # Generate random noise\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    rand = random.random()\n",
    "                    if rand < noise_probability / 2:\n",
    "                        image_array[i, j] = [0, 0, 0]  # Pepper noise\n",
    "                    elif rand < noise_probability:\n",
    "                        image_array[i, j] = [255, 255, 255]  # Salt noise\n",
    "\n",
    "            # Convert the numpy array back to an image\n",
    "            noisy_image = Image.fromarray(image_array)\n",
    "\n",
    "            file_extension = os.path.splitext(filename)[-1]\n",
    "\n",
    "            # Save the cut image with a new filename in the output folder\n",
    "            new_filename = filename.replace(file_extension, f\"_salt_and_pepper{file_extension}\")\n",
    "            new_image_path = os.path.join(output_folder, new_filename)\n",
    "            # Save the resulting image\n",
    "            noisy_image.save(new_image_path)\n",
    "\n",
    "            # Close the input and output images\n",
    "            image.close()\n",
    "            noisy_image.close()\n",
    "        else:\n",
    "            # Get the full image path\n",
    "            image_path = os.path.join(foldername, filename)\n",
    "            # Open the image\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # Save the original image to the output folder\n",
    "            original_image_path = os.path.join(output_folder, filename)\n",
    "            image.save(original_image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is normalised, it is time to split the dataset and train the Convolutional Neural Network (CNN) and the Support Vector Machine (SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16  # Pre-trained model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the data into training, test and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7e950b162486>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Copy the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Images have been split and copied to the train, validation, and test folders.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;31m# macOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the source folder containing subfolders with images\n",
    "source_folder = \".\\\\data\\\\muscima_pp_v2_symbols_resized_rotated\"\n",
    "\n",
    "# Define the destination folders\n",
    "train_folder = \".\\\\data\\\\train\"\n",
    "validation_folder = \".\\\\data\\\\validation\"\n",
    "test_folder = \".\\\\data\\\\test\"\n",
    "\n",
    "# Define the split ratios (80% train, 10% validation, 10% test)\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Iterate through subfolders in the source folder\n",
    "for root, dirs, files in os.walk(source_folder):\n",
    "    # Calculate the number of images to copy for each split\n",
    "    num_files = len(files)\n",
    "    num_train = int(num_files * train_ratio)\n",
    "    num_validation = int(num_files * validation_ratio)\n",
    "    num_test = num_files - num_train - num_validation\n",
    "\n",
    "    # Randomly shuffle the files in each subfolder\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Copy files to the destination folders based on the split ratios\n",
    "    for i, file_name in enumerate(files):\n",
    "        src_path = os.path.join(root, file_name)\n",
    "        if i < num_train:\n",
    "            dest_path = os.path.join(train_folder, root[len(source_folder) + 1:], file_name)\n",
    "        elif i < num_train + num_validation:\n",
    "            dest_path = os.path.join(validation_folder, root[len(source_folder) + 1:], file_name)\n",
    "        else:\n",
    "            dest_path = os.path.join(test_folder, root[len(source_folder) + 1:], file_name)\n",
    "\n",
    "        # Ensure the destination directory exists\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "\n",
    "        # Copy the file\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "print(\"Images have been split and copied to the train, validation, and test folders.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make the image generators from the different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 35\n",
      "Found 27361 images belonging to 35 classes.\n",
      "Found 3438 images belonging to 35 classes.\n",
      "Found 3454 images belonging to 35 classes.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to your dataset directory\n",
    "train_dir = '.\\\\data\\\\train_salt_and_pepper'\n",
    "test_dir = '.\\\\data\\\\test'\n",
    "validation_dir = '.\\\\data\\\\validation'\n",
    "\n",
    "\n",
    "# Define the size of your input images\n",
    "input_size = (344, 126)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator for validation and test data (without augmentation)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and split the data into train, validation, and test sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Specify training subset\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build and compile the CNN model. This will be done by using a pre-trained model for the feature extraction layers, and a softmax layer for the final classification layer.\n",
    "\n",
    "For the SVM, the same pre-trained model will be used, but the classification layer will be a SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(344, 126, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "# The final softmax layer for the CNN classifier\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "output_svm = Dense(num_classes, activation='linear')(x)\n",
    "\n",
    "f1score = tfa.metrics.F1Score(num_classes=num_classes, average='weighted')\n",
    "\n",
    "cnn = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "svm = Model(inputs=base_model.input, outputs=output_svm)\n",
    "\n",
    "cnn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', \n",
    "                    tf.keras.metrics.Precision(), \n",
    "                    tf.keras.metrics.Recall(), \n",
    "                    f1score,\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.TopKCategoricalAccuracy(k=5)])\n",
    "\n",
    "svm.compile(optimizer='adam',\n",
    "            loss='hinge',\n",
    "            metrics=['accuracy',\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                    f1score,\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.TopKCategoricalAccuracy(k=5)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation, we can train the model using the test and validation sets. These results are then stored in cnn_history. After this, the model is evaluated using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:b2n4tqee) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92b1a5108d04a958695983bf51ba942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='6258.442 MB of 6258.442 MB uploaded (4599.071 MB deduped)\\r'), FloatProgress(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████▇██████</td></tr><tr><td>batch/auc</td><td>▁▅▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>batch/f1_score</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇█████████████</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>█▅▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/precision</td><td>▁▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>batch/recall</td><td>▁▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇██▇██▇▇▇████████████████</td></tr><tr><td>batch/top_k_categorical_accuracy</td><td>▁▄▅▆▆▆▇▇████████████████████████████████</td></tr><tr><td>epoch/accuracy</td><td>▁▅▇██</td></tr><tr><td>epoch/auc</td><td>▁▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch/f1_score</td><td>▁▆▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▁▁</td></tr><tr><td>epoch/precision</td><td>▁▄▆██</td></tr><tr><td>epoch/recall</td><td>▁▆▇██</td></tr><tr><td>epoch/top_k_categorical_accuracy</td><td>▁▆▇██</td></tr><tr><td>epoch/val_accuracy</td><td>▁▇█▇█</td></tr><tr><td>epoch/val_auc</td><td>▁▆▇▆█</td></tr><tr><td>epoch/val_f1_score</td><td>▁▆█▇█</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▂▁</td></tr><tr><td>epoch/val_precision</td><td>▁▇█▇█</td></tr><tr><td>epoch/val_recall</td><td>▁▆▇▇█</td></tr><tr><td>epoch/val_top_k_categorical_accuracy</td><td>▁▂▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.6769</td></tr><tr><td>batch/auc</td><td>0.99047</td></tr><tr><td>batch/batch_step</td><td>4355</td></tr><tr><td>batch/f1_score</td><td>0.60035</td></tr><tr><td>batch/learning_rate</td><td>0.001</td></tr><tr><td>batch/loss</td><td>0.70428</td></tr><tr><td>batch/precision</td><td>0.70127</td></tr><tr><td>batch/recall</td><td>0.64844</td></tr><tr><td>batch/top_k_categorical_accuracy</td><td>0.98772</td></tr><tr><td>epoch/accuracy</td><td>0.66149</td></tr><tr><td>epoch/auc</td><td>0.98968</td></tr><tr><td>epoch/epoch</td><td>4</td></tr><tr><td>epoch/f1_score</td><td>0.60481</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.71207</td></tr><tr><td>epoch/precision</td><td>0.68833</td></tr><tr><td>epoch/recall</td><td>0.62984</td></tr><tr><td>epoch/top_k_categorical_accuracy</td><td>0.98666</td></tr><tr><td>epoch/val_accuracy</td><td>0.69488</td></tr><tr><td>epoch/val_auc</td><td>0.99249</td></tr><tr><td>epoch/val_f1_score</td><td>0.61296</td></tr><tr><td>epoch/val_loss</td><td>0.62202</td></tr><tr><td>epoch/val_precision</td><td>0.71131</td></tr><tr><td>epoch/val_recall</td><td>0.6751</td></tr><tr><td>epoch/val_top_k_categorical_accuracy</td><td>0.99564</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-yogurt-8</strong> at: <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/b2n4tqee' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/b2n4tqee</a><br/>Synced 5 W&B file(s), 0 media file(s), 55 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230928_080053-b2n4tqee\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:b2n4tqee). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b74616e16f941a68d0d2cc1e15e765f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888876342, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Reuben\\Documents\\NWU\\Honneurs\\Projek\\Artefak\\Recognition-of-Expressive-Symbols-in-Handwritten-Sheet-Music\\wandb\\run-20230928_125626-avnbz3gq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/avnbz3gq' target=\"_blank\">wise-dew-9</a></strong> to <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/avnbz3gq' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/avnbz3gq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.6680 - precision_3: 0.6931 - recall_3: 0.6404 - f1_score: 0.6073 - auc_3: 0.9902 - top_k_categorical_accuracy: 0.9876 INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 3.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 14926s 17s/step - loss: 0.6954 - accuracy: 0.6680 - precision_3: 0.6931 - recall_3: 0.6404 - f1_score: 0.6073 - auc_3: 0.9902 - top_k_categorical_accuracy: 0.9876 - val_loss: 0.5899 - val_accuracy: 0.7033 - val_precision_3: 0.7142 - val_recall_3: 0.6841 - val_f1_score: 0.6213 - val_auc_3: 0.9926 - val_top_k_categorical_accuracy: 0.9962\n",
      "Epoch 2/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.6770 - precision_3: 0.6994 - recall_3: 0.6496 - f1_score: 0.6070 - auc_3: 0.9906 - top_k_categorical_accuracy: 0.9891INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 2.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 4534s 5s/step - loss: 0.6780 - accuracy: 0.6770 - precision_3: 0.6994 - recall_3: 0.6496 - f1_score: 0.6070 - auc_3: 0.9906 - top_k_categorical_accuracy: 0.9891 - val_loss: 0.6057 - val_accuracy: 0.7030 - val_precision_3: 0.7175 - val_recall_3: 0.6847 - val_f1_score: 0.6206 - val_auc_3: 0.9920 - val_top_k_categorical_accuracy: 0.9959\n",
      "Epoch 3/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.6685 - accuracy: 0.6787 - precision_3: 0.7022 - recall_3: 0.6542 - f1_score: 0.6106 - auc_3: 0.9910 - top_k_categorical_accuracy: 0.9893INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 4985s 6s/step - loss: 0.6685 - accuracy: 0.6787 - precision_3: 0.7022 - recall_3: 0.6542 - f1_score: 0.6106 - auc_3: 0.9910 - top_k_categorical_accuracy: 0.9893 - val_loss: 0.6166 - val_accuracy: 0.7059 - val_precision_3: 0.7177 - val_recall_3: 0.6891 - val_f1_score: 0.6228 - val_auc_3: 0.9926 - val_top_k_categorical_accuracy: 0.9953\n",
      "Epoch 4/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.6599 - accuracy: 0.6824 - precision_3: 0.7050 - recall_3: 0.6579 - f1_score: 0.6132 - auc_3: 0.9908 - top_k_categorical_accuracy: 0.9914"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d08222f121ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m cnn_history = cnn.fit(\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1789\u001b[0m                             \u001b[0mpss_evaluation_shards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pss_evaluation_shards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m                         )\n\u001b[1;32m-> 1791\u001b[1;33m                     val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1792\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2198\u001b[0m                         ):\n\u001b[0;32m   2199\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2200\u001b[1;33m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[0;32m   2201\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2202\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   3998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3999\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4000\u001b[1;33m         \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4001\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4002\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    862\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m       (concrete_function,\n\u001b[0;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1348\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(project=\"Recognition of expressive symbols in handwritten sheet music\")\n",
    "\n",
    "wandb.config.epochs = 13\n",
    "wandb.config.batch_size = batch_size\n",
    "\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Train the model\n",
    "cnn_history = cnn.fit(\n",
    "    train_generator,\n",
    "    epochs=13,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stop, \n",
    "                WandbMetricsLogger(log_freq=5),\n",
    "                WandbModelCheckpoint(\"models\")]\n",
    ")\n",
    "\n",
    "\n",
    "#Evaluate the model using the test set\n",
    "test_results = cnn.evaluate(test_generator)\n",
    "print(\"Test loss:\", test_results[0])\n",
    "print(\"Test accuracy:\", test_results[1])\n",
    "\n",
    "wandb.log({\"test_loss\": test_results[0], \"test_accuracy\": test_results[1]})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the model can be saved for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('.\\\\models\\\\cnn_pre-trained_epoch4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.load_model(\".\\\\models\\\\cnn_pre-trained_2.keras\")\n",
    "\n",
    "cnn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy',\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                    f1score,\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.TopKCategoricalAccuracy(k=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = tf.keras.models.load_model(\".\\\\models\\\\svm_pre-trained_1.keras\")\n",
    "\n",
    "svm.compile(optimizer='adam',\n",
    "            loss='hinge',\n",
    "            metrics=['accuracy',\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                    f1score,\n",
    "                    tf.keras.metrics.AUC(),\n",
    "                    tf.keras.metrics.TopKCategoricalAccuracy(k=5)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zb1ibpx3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090d4ddb3fcb4e219ea9a64f435ad9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>batch/auc_1</td><td>▁▂▃▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>batch/f1_score</td><td>▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/precision_1</td><td>▁▁▂▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>batch/recall_1</td><td>▂▁▂▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>batch/top_k_categorical_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.54344</td></tr><tr><td>batch/auc_1</td><td>0.70348</td></tr><tr><td>batch/batch_step</td><td>855</td></tr><tr><td>batch/f1_score</td><td>0.45954</td></tr><tr><td>batch/learning_rate</td><td>0.001</td></tr><tr><td>batch/loss</td><td>0.05654</td></tr><tr><td>batch/precision_1</td><td>0.64147</td></tr><tr><td>batch/recall_1</td><td>0.30459</td></tr><tr><td>batch/top_k_categorical_accuracy</td><td>0.8704</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-rain-6</strong> at: <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/zb1ibpx3' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/zb1ibpx3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230927_111954-zb1ibpx3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zb1ibpx3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420bef550127431caee072123b92dd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888884685, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Reuben\\Documents\\NWU\\Honneurs\\Projek\\Artefak\\Recognition-of-Expressive-Symbols-in-Handwritten-Sheet-Music\\wandb\\run-20230927_144853-666jm9oj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/666jm9oj' target=\"_blank\">expert-firefly-7</a></strong> to <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/666jm9oj' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/666jm9oj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.6079 - precision_2: 0.6843 - recall_2: 0.4521 - f1_score: 0.5210 - auc_2: 0.7590 - top_k_categorical_accuracy: 0.9249INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 2.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2682s 3s/step - loss: 0.0463 - accuracy: 0.6079 - precision_2: 0.6843 - recall_2: 0.4521 - f1_score: 0.5210 - auc_2: 0.7590 - top_k_categorical_accuracy: 0.9249 - val_loss: 0.0427 - val_accuracy: 0.6463 - val_precision_2: 0.6920 - val_recall_2: 0.4802 - val_f1_score: 0.5604 - val_auc_2: 0.7666 - val_top_k_categorical_accuracy: 0.9514\n",
      "Epoch 2/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.6297 - precision_2: 0.6908 - recall_2: 0.4996 - f1_score: 0.5404 - auc_2: 0.7749 - top_k_categorical_accuracy: 0.9460INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2764s 3s/step - loss: 0.0438 - accuracy: 0.6297 - precision_2: 0.6908 - recall_2: 0.4996 - f1_score: 0.5404 - auc_2: 0.7749 - top_k_categorical_accuracy: 0.9460 - val_loss: 0.0385 - val_accuracy: 0.6684 - val_precision_2: 0.7132 - val_recall_2: 0.5844 - val_f1_score: 0.5775 - val_auc_2: 0.8052 - val_top_k_categorical_accuracy: 0.9695\n",
      "Epoch 3/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.6468 - precision_2: 0.7008 - recall_2: 0.5385 - f1_score: 0.5578 - auc_2: 0.7914 - top_k_categorical_accuracy: 0.9569INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2825s 3s/step - loss: 0.0417 - accuracy: 0.6468 - precision_2: 0.7008 - recall_2: 0.5385 - f1_score: 0.5578 - auc_2: 0.7914 - top_k_categorical_accuracy: 0.9569 - val_loss: 0.0391 - val_accuracy: 0.6803 - val_precision_2: 0.7128 - val_recall_2: 0.5890 - val_f1_score: 0.5897 - val_auc_2: 0.8092 - val_top_k_categorical_accuracy: 0.9814\n",
      "Epoch 4/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.6528 - precision_2: 0.7052 - recall_2: 0.5521 - f1_score: 0.5638 - auc_2: 0.7972 - top_k_categorical_accuracy: 0.9592INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2931s 3s/step - loss: 0.0412 - accuracy: 0.6528 - precision_2: 0.7052 - recall_2: 0.5521 - f1_score: 0.5638 - auc_2: 0.7972 - top_k_categorical_accuracy: 0.9592 - val_loss: 0.0372 - val_accuracy: 0.6812 - val_precision_2: 0.7263 - val_recall_2: 0.6213 - val_f1_score: 0.5881 - val_auc_2: 0.8238 - val_top_k_categorical_accuracy: 0.9788\n",
      "Epoch 5/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.6548 - precision_2: 0.7092 - recall_2: 0.5633 - f1_score: 0.5663 - auc_2: 0.8009 - top_k_categorical_accuracy: 0.9611INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 4.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2702s 3s/step - loss: 0.0406 - accuracy: 0.6548 - precision_2: 0.7092 - recall_2: 0.5633 - f1_score: 0.5663 - auc_2: 0.8009 - top_k_categorical_accuracy: 0.9611 - val_loss: 0.0391 - val_accuracy: 0.6754 - val_precision_2: 0.7197 - val_recall_2: 0.5974 - val_f1_score: 0.5900 - val_auc_2: 0.8168 - val_top_k_categorical_accuracy: 0.9770\n",
      "Epoch 6/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.6644 - precision_2: 0.7117 - recall_2: 0.5800 - f1_score: 0.5764 - auc_2: 0.8072 - top_k_categorical_accuracy: 0.9666INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2602s 3s/step - loss: 0.0395 - accuracy: 0.6644 - precision_2: 0.7117 - recall_2: 0.5800 - f1_score: 0.5764 - auc_2: 0.8072 - top_k_categorical_accuracy: 0.9666 - val_loss: 0.0374 - val_accuracy: 0.6786 - val_precision_2: 0.7223 - val_recall_2: 0.6303 - val_f1_score: 0.5852 - val_auc_2: 0.8264 - val_top_k_categorical_accuracy: 0.9741\n",
      "Epoch 7/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.6641 - precision_2: 0.7129 - recall_2: 0.5842 - f1_score: 0.5769 - auc_2: 0.8076 - top_k_categorical_accuracy: 0.9650INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2643s 3s/step - loss: 0.0394 - accuracy: 0.6641 - precision_2: 0.7129 - recall_2: 0.5842 - f1_score: 0.5769 - auc_2: 0.8076 - top_k_categorical_accuracy: 0.9650 - val_loss: 0.0377 - val_accuracy: 0.6792 - val_precision_2: 0.7284 - val_recall_2: 0.6099 - val_f1_score: 0.5890 - val_auc_2: 0.8249 - val_top_k_categorical_accuracy: 0.9727\n",
      "Epoch 8/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.6608 - precision_2: 0.7129 - recall_2: 0.5818 - f1_score: 0.5729 - auc_2: 0.8066 - top_k_categorical_accuracy: 0.9667INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2600s 3s/step - loss: 0.0395 - accuracy: 0.6608 - precision_2: 0.7129 - recall_2: 0.5818 - f1_score: 0.5729 - auc_2: 0.8066 - top_k_categorical_accuracy: 0.9667 - val_loss: 0.0368 - val_accuracy: 0.6847 - val_precision_2: 0.7230 - val_recall_2: 0.6323 - val_f1_score: 0.5945 - val_auc_2: 0.8266 - val_top_k_categorical_accuracy: 0.9785\n",
      "Epoch 9/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.6650 - precision_2: 0.7128 - recall_2: 0.5875 - f1_score: 0.5772 - auc_2: 0.8093 - top_k_categorical_accuracy: 0.9681INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2873s 3s/step - loss: 0.0390 - accuracy: 0.6650 - precision_2: 0.7128 - recall_2: 0.5875 - f1_score: 0.5772 - auc_2: 0.8093 - top_k_categorical_accuracy: 0.9681 - val_loss: 0.0382 - val_accuracy: 0.6763 - val_precision_2: 0.7176 - val_recall_2: 0.6105 - val_f1_score: 0.5870 - val_auc_2: 0.8212 - val_top_k_categorical_accuracy: 0.9776\n",
      "Epoch 10/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.6683 - precision_2: 0.7168 - recall_2: 0.5918 - f1_score: 0.5806 - auc_2: 0.8111 - top_k_categorical_accuracy: 0.9695INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2895s 3s/step - loss: 0.0385 - accuracy: 0.6683 - precision_2: 0.7168 - recall_2: 0.5918 - f1_score: 0.5806 - auc_2: 0.8111 - top_k_categorical_accuracy: 0.9695 - val_loss: 0.0365 - val_accuracy: 0.6914 - val_precision_2: 0.7267 - val_recall_2: 0.6227 - val_f1_score: 0.6030 - val_auc_2: 0.8238 - val_top_k_categorical_accuracy: 0.9808\n",
      "Epoch 11/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.6690 - precision_2: 0.7174 - recall_2: 0.5965 - f1_score: 0.5821 - auc_2: 0.8114 - top_k_categorical_accuracy: 0.9661INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 3.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2896s 3s/step - loss: 0.0387 - accuracy: 0.6690 - precision_2: 0.7174 - recall_2: 0.5965 - f1_score: 0.5821 - auc_2: 0.8114 - top_k_categorical_accuracy: 0.9661 - val_loss: 0.0385 - val_accuracy: 0.6693 - val_precision_2: 0.7221 - val_recall_2: 0.5881 - val_f1_score: 0.5834 - val_auc_2: 0.8084 - val_top_k_categorical_accuracy: 0.9639\n",
      "Epoch 12/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.6658 - precision_2: 0.7148 - recall_2: 0.5926 - f1_score: 0.5779 - auc_2: 0.8106 - top_k_categorical_accuracy: 0.9678INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 2816s 3s/step - loss: 0.0387 - accuracy: 0.6658 - precision_2: 0.7148 - recall_2: 0.5926 - f1_score: 0.5779 - auc_2: 0.8106 - top_k_categorical_accuracy: 0.9678 - val_loss: 0.0383 - val_accuracy: 0.6806 - val_precision_2: 0.7227 - val_recall_2: 0.5791 - val_f1_score: 0.5933 - val_auc_2: 0.8160 - val_top_k_categorical_accuracy: 0.9770\n",
      "Epoch 13/13\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.6748 - precision_2: 0.7194 - recall_2: 0.6024 - f1_score: 0.5872 - auc_2: 0.8149 - top_k_categorical_accuracy: 0.9705INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 3209s 4s/step - loss: 0.0379 - accuracy: 0.6748 - precision_2: 0.7194 - recall_2: 0.6024 - f1_score: 0.5872 - auc_2: 0.8149 - top_k_categorical_accuracy: 0.9705 - val_loss: 0.0377 - val_accuracy: 0.6847 - val_precision_2: 0.7200 - val_recall_2: 0.6102 - val_f1_score: 0.5975 - val_auc_2: 0.8220 - val_top_k_categorical_accuracy: 0.9814\n",
      "108/108 [==============================] - 459s 4s/step - loss: 5.5728 - accuracy: 0.0197 - precision: 0.0116 - recall: 2.8952e-04 - f1_score: 0.4029 - auc: 0.3551 - top_k_categorical_accuracy: 0.0510\n",
      "Test loss: 5.572769641876221\n",
      "Test accuracy: 0.019687319174408913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▁▂▄▄▄▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▆▇▆█▇▇█▇▇██▇▆▇▇▆▇█</td></tr><tr><td>batch/auc_2</td><td>▁▂▂▃▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇██▇███▇▇▇▇▇█</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>batch/f1_score</td><td>▁▁▁▃▃▃▄▅▅▅▅▅▅▅▅▇▆▆▇▇▇▇▆▆▆█▇▇█▇▇██▇▅▆▇▆▇▇</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>█▇▇▅▅▅▄▃▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▂▂▂▂▁▁</td></tr><tr><td>batch/precision_2</td><td>▂▂▁▄▃▃▁▅▄▄▄▄▅▅▅▇▅▅▆▆▆▇▆▆▆█▆▆█▇▇█▇▇▄▆▆▅▇▇</td></tr><tr><td>batch/recall_2</td><td>▁▂▃▃▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇██▇███▇▇▇▇██</td></tr><tr><td>batch/top_k_categorical_accuracy</td><td>▁▂▃▅▅▅▆▆▆▆▇▆▇▆▇▆▇▇▇██▇▇▇▇██▇██████▇▇▇▇██</td></tr><tr><td>epoch/accuracy</td><td>▁▃▅▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>epoch/auc_2</td><td>▁▃▅▆▆▇▇▇▇██▇█</td></tr><tr><td>epoch/epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>epoch/f1_score</td><td>▁▃▅▆▆▇▇▆▇▇▇▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▄▄▃▂▂▂▂▂▂▂▁</td></tr><tr><td>epoch/precision_2</td><td>▁▂▄▅▆▆▇▇▇▇█▇█</td></tr><tr><td>epoch/recall_2</td><td>▁▃▅▆▆▇▇▇▇████</td></tr><tr><td>epoch/top_k_categorical_accuracy</td><td>▁▄▆▆▇▇▇▇██▇██</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▆▆▆▆▆▇▆█▅▆▇</td></tr><tr><td>epoch/val_auc_2</td><td>▁▆▆█▇███▇█▆▇▇</td></tr><tr><td>epoch/val_f1_score</td><td>▁▄▆▆▆▅▆▇▅█▅▆▇</td></tr><tr><td>epoch/val_loss</td><td>█▃▄▂▄▂▂▁▃▁▃▃▂</td></tr><tr><td>epoch/val_precision_2</td><td>▁▅▅█▆▇█▇▆█▇▇▆</td></tr><tr><td>epoch/val_recall_2</td><td>▁▆▆▇▆█▇█▇█▆▆▇</td></tr><tr><td>epoch/val_top_k_categorical_accuracy</td><td>▁▅█▇▇▆▆▇▇█▄▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.67476</td></tr><tr><td>batch/auc_2</td><td>0.81487</td></tr><tr><td>batch/batch_step</td><td>11175</td></tr><tr><td>batch/f1_score</td><td>0.58721</td></tr><tr><td>batch/learning_rate</td><td>0.001</td></tr><tr><td>batch/loss</td><td>0.03787</td></tr><tr><td>batch/precision_2</td><td>0.71941</td></tr><tr><td>batch/recall_2</td><td>0.60235</td></tr><tr><td>batch/top_k_categorical_accuracy</td><td>0.97047</td></tr><tr><td>epoch/accuracy</td><td>0.67476</td></tr><tr><td>epoch/auc_2</td><td>0.81487</td></tr><tr><td>epoch/epoch</td><td>12</td></tr><tr><td>epoch/f1_score</td><td>0.58721</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.03787</td></tr><tr><td>epoch/precision_2</td><td>0.71941</td></tr><tr><td>epoch/recall_2</td><td>0.60235</td></tr><tr><td>epoch/top_k_categorical_accuracy</td><td>0.97047</td></tr><tr><td>epoch/val_accuracy</td><td>0.6847</td></tr><tr><td>epoch/val_auc_2</td><td>0.82197</td></tr><tr><td>epoch/val_f1_score</td><td>0.5975</td></tr><tr><td>epoch/val_loss</td><td>0.0377</td></tr><tr><td>epoch/val_precision_2</td><td>0.71997</td></tr><tr><td>epoch/val_recall_2</td><td>0.61024</td></tr><tr><td>epoch/val_top_k_categorical_accuracy</td><td>0.98138</td></tr><tr><td>test_accuracy</td><td>0.01969</td></tr><tr><td>test_loss</td><td>5.57277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">expert-firefly-7</strong> at: <a href='https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/666jm9oj' target=\"_blank\">https://wandb.ai/reubens-team/Recognition%20of%20expressive%20symbols%20in%20handwritten%20sheet%20music/runs/666jm9oj</a><br/>Synced 5 W&B file(s), 0 media file(s), 130 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230927_144853-666jm9oj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(project=\"Recognition of expressive symbols in handwritten sheet music\")\n",
    "\n",
    "wandb.config.epochs = 13\n",
    "wandb.config.batch_size = batch_size\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "svm_history = svm.fit(\n",
    "    train_generator,\n",
    "    epochs=13,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stop, \n",
    "                WandbMetricsLogger(log_freq=5),\n",
    "                WandbModelCheckpoint(\"models\")]\n",
    ")\n",
    "\n",
    "#Evaluate the model using the test set\n",
    "test_results = cnn.evaluate(test_generator)\n",
    "print(\"Test loss:\", test_results[0])\n",
    "print(\"Test accuracy:\", test_results[1])\n",
    "\n",
    "wandb.log({\"test_loss\": test_results[0], \"test_accuracy\": test_results[1]})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 243s 2s/step - loss: 0.0380 - accuracy: 0.6789 - precision_2: 0.7184 - recall_2: 0.6129 - f1_score: 0.5923 - auc_2: 0.8213 - top_k_categorical_accuracy: 0.9835\n",
      "Test loss: 0.03796811401844025\n",
      "Test accuracy: 0.6789230108261108\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model using the test set\n",
    "test_results = svm.evaluate(test_generator)\n",
    "print(\"Test loss:\", test_results[0])\n",
    "print(\"Test accuracy:\", test_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.save('.\\\\models\\\\svm_pre-trained_2.keras')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use matplotlib to visualise the training progress by using the \"cnn_history\" variable to plot the training and validation accuracy and loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(svm_history.history['accuracy'])\n",
    "plt.plot(svm_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(svm_history.history['loss'])\n",
    "plt.plot(svm_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
